"""
Tests for integration pathways (inter-region connections).

These tests verify that pathways:
1. Can be instantiated with proper configuration
2. Process spikes through forward() correctly
3. Apply learning rules (STDP, BCM, etc.)
4. Handle bool spike inputs/outputs
5. Maintain state across timesteps

Pathways tested:
- SpikingPathway (base class with STDP)
- SpikingAttentionPathway (PFC→Cortex top-down modulation)
- SpikingReplayPathway (Hippocampus→Cortex consolidation)
"""

import pytest
import torch

from thalia.integration import SpikingPathway, SpikingPathwayConfig
from thalia.integration.pathways import (
    SpikingAttentionPathway,
    SpikingAttentionPathwayConfig,
    SpikingReplayPathway,
    SpikingReplayPathwayConfig,
)


class TestSpikingPathway:
    """Tests for the base SpikingPathway (STDP-based inter-region connection)."""

    @pytest.fixture
    def pathway_config(self):
        """Basic pathway configuration."""
        return SpikingPathwayConfig(
            source_size=64,
            target_size=32,
            stdp_lr=0.01,
            tau_plus_ms=20.0,
            tau_minus_ms=20.0
        )

    @pytest.fixture
    def pathway(self, pathway_config):
        """Create a pathway instance."""
        return SpikingPathway(pathway_config)

    def test_initialization(self, pathway, pathway_config):
        """Test that pathway initializes correctly."""
        assert pathway.weights.shape == (pathway_config.target_size, pathway_config.source_size)
        assert pathway.config.stdp_lr == pathway_config.stdp_lr

        # Check traces exist
        assert pathway.pre_trace is not None
        assert pathway.post_trace is not None
        # Traces may be squeezed for single samples
        assert pathway.pre_trace.shape[-1] == pathway_config.source_size
        assert pathway.post_trace.shape[-1] == pathway_config.target_size

    def test_forward_pass_float_input(self, pathway, pathway_config):
        """Test forward pass with float input (backward compatibility)."""
        batch_size = 1
        input_spikes = torch.randint(0, 2, (batch_size, pathway_config.source_size)).float()

        output = pathway.forward(input_spikes)

        # Pathway may squeeze batch dimension for single samples
        assert output.shape[-1] == pathway_config.target_size
        # Output should be float for backward compatibility
        assert output.dtype in [torch.float32, torch.float64]

    def test_forward_pass_bool_input(self, pathway, pathway_config):
        """Test forward pass with bool input (new bool spike standard)."""
        batch_size = 1
        input_spikes = torch.randint(0, 2, (batch_size, pathway_config.source_size), dtype=torch.bool)

        output = pathway.forward(input_spikes)

        # Pathway may squeeze batch dimension for single samples
        assert output.shape[-1] == pathway_config.target_size
        # Pathway should handle bool input gracefully

    def test_stdp_learning(self, pathway):
        """Test that STDP updates weights based on spike timing."""
        batch_size = 1

        # Record initial weights
        initial_weights = pathway.weights.data.clone()

        # Create pre→post causal spike pattern (should cause LTP)
        # Pre fires first (t=0), then post (t=1)
        pre_spikes = torch.zeros(batch_size, 64)
        pre_spikes[:, :10] = 1.0  # First 10 neurons fire

        post_spikes = torch.zeros(batch_size, 32)
        post_spikes[:, :5] = 1.0  # First 5 neurons fire

        # Simulate pre→post timing
        pathway.forward(pre_spikes)  # Pre fires, builds up pre_trace
        pathway.forward(torch.zeros_like(pre_spikes))  # Delay
        # Now post fires when pre_trace is still high → LTP

        # Manually trigger post spikes to test learning
        # (In real usage, post region would produce these)
        pathway.post_trace = pathway.post_trace * 0.9 + post_spikes.squeeze()  # Simulate post spike
        pathway._apply_stdp(pre_spikes.squeeze(), post_spikes.squeeze(), dt=1.0)

        # Weights should have changed
        weight_change = (pathway.weights.data - initial_weights).abs().sum()
        assert weight_change > 0, "STDP should modify weights"

    def test_trace_buildup(self, pathway):
        """Test that spike traces build up over time."""
        batch_size = 1
        input_spikes = torch.ones(batch_size, 64)  # Continuous activity

        # Initial traces should be zero
        assert pathway.pre_trace.abs().sum() == 0

        # Run forward multiple times
        for _ in range(5):
            pathway.forward(input_spikes)

        # Pre trace should have accumulated
        assert pathway.pre_trace.abs().sum() > 0

    def test_trace_decay(self, pathway):
        """Test that traces decay when there's no activity."""
        batch_size = 1

        # Build up trace with activity
        input_spikes = torch.ones(batch_size, 64)
        for _ in range(5):
            pathway.forward(input_spikes)

        initial_trace = pathway.pre_trace.clone()

        # Run with no input
        zero_input = torch.zeros(batch_size, 64)
        for _ in range(10):
            pathway.forward(zero_input)

        # Trace should have decayed
        assert pathway.pre_trace.abs().sum() < initial_trace.abs().sum()

    def test_weight_normalization(self, pathway):
        """Test that weights stay within bounds."""
        batch_size = 1

        # Run many learning iterations to try to saturate weights
        for _ in range(100):
            input_spikes = torch.randint(0, 2, (batch_size, 64)).float()
            output = pathway.forward(input_spikes)
            # Simulate learning (this would normally happen in pathway)

        # Weights should be bounded
        assert pathway.weights.min() >= 0, "Weights should be non-negative"
        assert pathway.weights.max() <= 10.0, "Weights shouldn't explode"


class TestSpikingAttentionPathway:
    """Tests for SpikingAttentionPathway (PFC→Cortex top-down modulation)."""

    @pytest.fixture
    def attention_config(self):
        """Basic attention pathway configuration."""
        return SpikingAttentionPathwayConfig(
            source_size=32,   # PFC working memory
            target_size=64,  # Cortex features
            attention_gain=2.0,
            stdp_lr=0.01
        )

    @pytest.fixture
    def attention_pathway(self, attention_config):
        """Create an attention pathway instance."""
        return SpikingAttentionPathway(attention_config)

    def test_initialization(self, attention_pathway, attention_config):
        """Test that attention pathway initializes correctly."""
        assert attention_pathway.weights.shape == (attention_config.target_size, attention_config.source_size)
        assert hasattr(attention_pathway, 'attention_gain') or hasattr(attention_pathway.config, 'attention_gain')
        assert attention_pathway.config.attention_gain == attention_config.attention_gain

    def test_attention_modulation(self, attention_pathway):
        """Test that PFC activity modulates cortical processing."""
        # PFC activity pattern (what we're attending to)
        pfc_spikes = torch.zeros(32, dtype=torch.bool)
        pfc_spikes[:5] = True  # Attending to first 5 features

        # Forward pass produces attention signal
        attention_output = attention_pathway.forward(pfc_spikes)

        assert attention_output.shape == (64,)
        # Attention should be present (non-zero)
        assert attention_output.abs().sum() > 0

    def test_attention_gain_scaling(self, attention_pathway):
        """Test that attention_gain parameter scales modulation."""
        pfc_spikes = torch.ones(32, dtype=torch.bool)

        # Get attention with default gain
        output1 = attention_pathway.forward(pfc_spikes).clone()

        # Change attention gain
        attention_pathway.config.attention_gain = 5.0
        output2 = attention_pathway.forward(pfc_spikes).clone()

        # Stronger attention should produce different output
        # (exact relationship depends on implementation)
        assert not torch.equal(output1, output2)

    def test_attention_learning(self, attention_pathway):
        """Test that attention pathway learns PFC→Cortex associations."""
        initial_weights = attention_pathway.weights.data.clone()

        # Repeated PFC→Cortex activity should strengthen connections
        pfc_pattern = torch.zeros(32, dtype=torch.bool)
        pfc_pattern[10:15] = True

        for _ in range(10):
            attention_pathway.forward(pfc_pattern)

        # Weights should have changed (learning occurred)
        weight_change = (attention_pathway.weights.data - initial_weights).abs().sum()
        assert weight_change > 0, "Attention pathway should learn"


class TestSpikingReplayPathway:
    """Tests for SpikingReplayPathway (Hippocampus→Cortex consolidation during sleep)."""

    @pytest.fixture
    def replay_config(self):
        """Basic replay pathway configuration."""
        return SpikingReplayPathwayConfig(
            source_size=32,   # Hippocampus output
            target_size=64,  # Cortex features
            replay_gain=3.0,
            stdp_lr=0.05
        )

    @pytest.fixture
    def replay_pathway(self, replay_config):
        """Create a replay pathway instance."""
        return SpikingReplayPathway(replay_config)

    def test_initialization(self, replay_pathway, replay_config):
        """Test that replay pathway initializes correctly."""
        assert replay_pathway.weights.shape == (replay_config.target_size, replay_config.source_size)
        assert hasattr(replay_pathway, 'replay_gain') or hasattr(replay_pathway.config, 'replay_gain')
        assert replay_pathway.config.replay_gain == replay_config.replay_gain

    def test_replay_forward_pass(self, replay_pathway):
        """Test that hippocampal replay drives cortical activity."""
        # Hippocampal replay pattern
        hippo_spikes = torch.randint(0, 2, (32,), dtype=torch.bool)

        # Forward pass produces cortical reactivation
        cortex_reactivation = replay_pathway.forward(hippo_spikes)

        assert cortex_reactivation.shape == (64,)
        # Replay should produce output when there's input
        if hippo_spikes.sum() > 0:
            assert cortex_reactivation.abs().sum() > 0

    def test_replay_gain_scaling(self, replay_pathway):
        """Test that replay_gain controls consolidation strength."""
        hippo_pattern = torch.ones(32, dtype=torch.bool)

        # Weak replay
        replay_pathway.config.replay_gain = 1.0
        weak_output = replay_pathway.forward(hippo_pattern).clone()

        # Strong replay
        replay_pathway.config.replay_gain = 5.0
        strong_output = replay_pathway.forward(hippo_pattern).clone()

        # Stronger replay should produce larger output
        assert strong_output.abs().sum() > weak_output.abs().sum()

    def test_replay_consolidation_learning(self, replay_pathway):
        """Test that repeated replay strengthens Hippo→Cortex connections."""
        initial_weights = replay_pathway.weights.data.clone()

        # Simulate sleep replay: hippocampus replays pattern multiple times
        hippo_memory = torch.zeros(32, dtype=torch.bool)
        hippo_memory[5:10] = True  # Specific memory pattern

        # Replay 20 times (simulating sharp-wave ripples during sleep)
        for _ in range(20):
            replay_pathway.forward(hippo_memory)

        # Weights should have changed (consolidation occurred)
        weight_change = (replay_pathway.weights.data - initial_weights).abs().sum()
        assert weight_change > 0, "Replay should consolidate memories"

    def test_replay_pattern_specificity(self, replay_pathway):
        """Test that replay transfers specific patterns, not general activation."""
        # Two different hippocampal patterns
        pattern_A = torch.zeros(32, dtype=torch.bool)
        pattern_A[:10] = True

        pattern_B = torch.zeros(32, dtype=torch.bool)
        pattern_B[20:30] = True

        # Replay should produce different cortical outputs
        output_A = replay_pathway.forward(pattern_A)
        output_B = replay_pathway.forward(pattern_B)

        # Outputs should differ (pattern-specific)
        assert not torch.equal(output_A > 0, output_B > 0), \
            "Different hippocampal patterns should produce different cortical reactivations"


class TestPathwayIntegration:
    """Integration tests for multiple pathways working together."""

    def test_pathway_composition(self):
        """Test that multiple pathways can connect same regions."""
        # Create two pathways from different sources to same target
        pathway1 = SpikingPathway(SpikingPathwayConfig(
            source_size=32, target_size=64
        ))
        pathway2 = SpikingPathway(SpikingPathwayConfig(
            source_size=32, target_size=64
        ))

        input1 = torch.randint(0, 2, (32,), dtype=torch.bool)
        input2 = torch.randint(0, 2, (32,), dtype=torch.bool)

        # Both pathways can provide input to same target
        output1 = pathway1.forward(input1)
        output2 = pathway2.forward(input2)

        # Outputs can be combined (additive in real brain)
        combined = output1 + output2
        assert combined.shape == (64,)

    def test_attention_and_replay_together(self):
        """Test that attention and replay pathways can coexist."""
        attention = SpikingAttentionPathway(SpikingAttentionPathwayConfig(
            source_size=32, target_size=64
        ))
        replay = SpikingReplayPathway(SpikingReplayPathwayConfig(
            source_size=32, target_size=64
        ))

        pfc_input = torch.randint(0, 2, (32,), dtype=torch.bool)
        hippo_input = torch.randint(0, 2, (32,), dtype=torch.bool)

        # Both can operate simultaneously
        attention_signal = attention.forward(pfc_input)
        replay_signal = replay.forward(hippo_input)

        # During wake: attention dominates
        wake_input = attention_signal * 1.0 + replay_signal * 0.0

        # During sleep: replay dominates
        sleep_input = attention_signal * 0.0 + replay_signal * 1.0

        assert wake_input.shape == sleep_input.shape

    def test_bidirectional_pathways(self):
        """Test that bidirectional connections can be implemented."""
        # Forward pathway: A → B
        forward = SpikingPathway(SpikingPathwayConfig(
            source_size=32, target_size=64
        ))

        # Backward pathway: B → A (feedback)
        backward = SpikingPathway(SpikingPathwayConfig(
            source_size=64, target_size=32
        ))

        # Forward pass
        a_activity = torch.randint(0, 2, (32,), dtype=torch.bool)
        b_activity = forward.forward(a_activity)

        # Backward pass (feedback)
        a_feedback = backward.forward(b_activity)

        assert a_feedback.shape == a_activity.shape


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
