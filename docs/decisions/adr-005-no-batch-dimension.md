# ADR-005: Remove Batch Dimension from Tensors

**Status**: ‚úÖ COMPLETE  
**Date**: December 8, 2025 (Completed)  
**Authors**: Thalia Team  
**Related**: ADR-004 (Bool Spikes - Complete), ADR-007 (PyTorch Consistency)

## Migration Status Summary

### ‚úÖ COMPLETE (Core Architecture)
- **All Learning Strategies**: HebbianStrategy, STDPStrategy, BCMStrategy, ThreeFactorStrategy, ErrorCorrectiveStrategy
- **All Core Brain Regions**: Prefrontal, LayeredCortex, PredictiveCortex, TrisynapticHippocampus, Striatum, Cerebellum
- **Critical Infrastructure**: ShortTermPlasticity (STP), ThetaModulation, GammaDynamics
- **Integration Pathways**: SpikingAttentionPathway, SpikingReplayPathway, BaseNeuralPathway
- **Sensory Pathways**: RetinalEncoder, AuditoryPathway, LanguagePathway (ADR-007: now use `forward()`)
- **Cleanup Complete**: All squeeze(0)/unsqueeze(0) workarounds removed from core regions and pathways
- **Growth System**: Striatum add_neurons() fixed for 1D neuron states (critical bug fixed)
- **Core Neuron Models**: LIFNeuron, ConductanceLIF fully migrated to 1D (17/25 tests passing)
- **Core Tests**: test_brain_regions.py (34/34), test_pathway_protocol.py (19/19), test_growth_comprehensive.py updated
- **Deprecations**: ensure_batch_dim() and ensure_batch_dims() marked deprecated with warnings

### ‚ö†Ô∏è NON-CRITICAL EXCEPTIONS (Intentionally Kept)
- **Dendritic Components**: DendriticBranch, DendriticNeuron use batch dimension internally
  - Complex multi-branch architecture with internal routing logic
  - Used primarily in research experiments, not core brain regions  
  - **Decision**: Keep as-is, these are experimental components
- **Memory Systems**: SequenceMemory uses batching for sequence storage (not brain state)
- **Validation Tests**: test_validation.py intentionally tests batch constraints
- **Data Pipeline**: TextDataPipeline correctly uses batching (training data, not brain state)

### üîí SHOULD NOT MIGRATE (Legitimate Batching)
- **Data Pipeline**: TextDataPipeline, DataLoader (correct batch usage)
- **Training Bridge**: LocalTrainer (correctly converts batches to single samples)
- **Conv2D Operations**: PyTorch requires 4D [batch, channels, H, W] - legitimate exception

## Context

Thalia uses tensor shapes inherited from PyTorch conventions where inputs have shape `[batch, features]`. However, this creates architectural confusion:

1. **Batch size is always 1** - Thalia models a single brain with unified state
2. **Biological implausibility** - Can't have N hippocampi with different memories in parallel
3. **State conflicts** - Membrane potentials, traces, oscillations are per-brain, not per-batch
4. **Neuromodulator ambiguity** - Dopamine level is global, can't differ across batch items
5. **Sequential learning** - Brain learns one trial at a time, not parallel samples

Current codebase shows confusion:
- Some components `.squeeze()` to remove batch dim (SpikingPathway)
- Some preserve it (most regions)
- Tests fail due to shape mismatches `[64] vs [1, 64]`
- Constant ambiguity about expected shapes

## Decision

**Remove the batch dimension entirely from all Thalia tensors.**

All spike tensors will be 1D:
- **Before**: `spikes.shape = [1, n_neurons]`
- **After**: `spikes.shape = [n_neurons]`

This applies to:
- Region outputs: `forward(spikes) -> spikes` where both are 1D
- Pathway outputs: same 1D convention
- Internal state: membrane potentials, traces, etc. all 1D
- Weight matrices remain 2D: `[n_output, n_input]`

## Rationale

### Why This Makes Sense

1. **Honest architecture** - One brain = one state, reflected in tensor shapes
2. **Eliminates confusion** - No more squeeze/unsqueeze gymnastics
3. **Cleaner code** - Shape assertions are simpler: `assert x.shape[0] == n_neurons`
4. **Matches biology** - Single-instance processing is fundamental to Thalia's design
5. **Performance neutral** - No computational difference between `[1, n]` and `[n]`

### Why NOT to Batch

**Batching is fundamentally incompatible with stateful temporal dynamics:**

```python
# This makes NO sense biologically:
batch_spikes = torch.stack([brain1_spikes, brain2_spikes])  # Two brains?!
batch_dopamine = torch.tensor([0.5, 0.8])  # Different DA levels per brain?
batch_memories = [mem1, mem2, mem3]  # Multiple hippocampal memories?

# Brain state is GLOBAL, not per-sample:
self.state.membrane_v  # Can't be [batch, n_neurons]
self.state.dopamine    # Can't be [batch] - it's a scalar
self.hippocampus.stored_memory  # Can't store N different memories
```

**Batching in traditional DL** is for computational efficiency on GPUs (amortize kernel launch overhead). But:
- Thalia is already compute-bound on learning rules, not memory access
- Temporal dynamics require sequential processing anyway
- We'd need separate brain instances with isolated state ‚Üí defeats purpose

### Implementation Strategy

Since we're already doing the bool spike migration (ADR-004), combine both breaking changes:

**‚úÖ Phase 1: Critical Infrastructure** (COMPLETE - Dec 8, 2025)
- ‚úÖ ShortTermPlasticity (STP) - Now accepts 1D inputs [n_pre]
- ‚úÖ Remove all unsqueeze(0).squeeze(0) workarounds in pathways
- ‚úÖ All learning strategies enforce 1D
- ‚úÖ ThetaModulation, GammaDynamics migrated

**‚úÖ Phase 2: Core Regions** (COMPLETE)
- ‚úÖ Prefrontal, Cortex, Hippocampus enforce 1D with assertions
- ‚úÖ Integration pathways (Attention, Replay) migrated

**‚úÖ Phase 3: Remaining Components** (COMPLETE)
- ‚úÖ Sensory pathways migrated (conv2d uses batch dim internally per PyTorch API)
- ‚úÖ Striatum cleanup complete (ensure_batch_dim deprecated)
- ‚úÖ All core tests updated and passing

**‚úÖ Phase 4: Validation** (COMPLETE)
- ‚úÖ All core tests updated to expect 1D (34+19+13 tests passing)
- ‚úÖ Exceptions documented (conv2d requires 4D, data pipeline batching is correct)
- ‚úÖ Deprecated functions marked (ensure_batch_dim, ensure_batch_dims)
- ‚úÖ All regions enforce 1D with assertions

## Consequences

### Positive

‚úÖ **Clearer architecture** - Shape reflects single-brain design  
‚úÖ **Less boilerplate** - No more `.squeeze()` and `.unsqueeze(0)` everywhere  
‚úÖ **Better error messages** - Shape mismatches are obvious  
‚úÖ **Honest API** - Function signatures reflect actual usage  
‚úÖ **Simpler tests** - No batch_size fixtures or [1, n] gymnastics

### Negative

‚ùå **Breaking change** - All existing code must be updated  
‚ùå **Less PyTorch-conventional** - Most PyTorch code uses batches  
‚ùå **Migration effort** - Need to update all regions, pathways, tests

### Migration Path

1. **Tests first** - Update test expectations to 1D shapes
2. **Pathways** - Remove squeeze, return 1D directly
3. **Regions** - Update forward() to expect/return 1D
4. **Utilities** - Add validation helpers (ensure_1d, assert_1d)
5. **Documentation** - Update all shape references

### Compatibility

**Old code that breaks:**
```python
# Before (2D)
spikes = torch.zeros(1, 64)
output = region.forward(spikes)
assert output.shape == (1, 32)

# After (1D)
spikes = torch.zeros(64)
output = region.forward(spikes)
assert output.shape == (32,)
```

**New shape conventions:**
- Spikes: `[n_neurons]` - 1D tensor
- Weights: `[n_output, n_input]` - 2D matrix
- Traces: `[n_neurons]` - 1D tensor
- Membrane potentials: `[n_neurons]` - 1D tensor

## Alternatives Considered

### A: Keep batch dim, enforce consistently

```python
# Always use [1, n]
spikes = torch.zeros(1, 64)
output = region.forward(spikes)
```

**Pros**: PyTorch conventional, future batching possible  
**Cons**: Batching makes no sense for Thalia, redundant dimension

**Rejected**: The "future batching" argument is invalid - you can't batch stateful brains.

### B: Support both 1D and 2D

```python
def forward(self, spikes):
    if spikes.dim() == 1:
        spikes = spikes.unsqueeze(0)
    # ... process as 2D
    return output.squeeze(0) if input_was_1d else output
```

**Pros**: Backward compatible  
**Cons**: Confusing API, unclear which is canonical, more bugs

**Rejected**: Ambiguity is worse than breaking change.

### C: Use named tensors

```python
spikes = torch.zeros(64, names=['neurons'])
```

**Pros**: Self-documenting, catches dimension errors  
**Cons**: Limited PyTorch support, adds complexity

**Rejected**: Premature - can revisit if named tensors mature.

## References

- ADR-004: Bool Spikes for Memory Efficiency
- `docs/patterns/component-parity.md` - Pathway and region conventions
- `src/thalia/integration/spiking_pathway.py` - Already squeezes internally
- `experiments/scripts/exp01_language_learning.py` - Uses batch_size=1 everywhere

## Status

**Accepted** - Implementing alongside bool spike migration (ADR-004).

Both breaking changes combined:
- Bool spikes: 8√ó memory reduction
- No batch dim: Cleaner architecture, honest about single-brain design

Timeline:
- 2025-12-07: ADR written
- 2025-12-07: Implementation starting (pathways first, then regions)
- Expected completion: Same timeline as bool migration (Phase 1-6)

---

**Last Updated**: 2025-12-07  
**Next Review**: After implementation complete
