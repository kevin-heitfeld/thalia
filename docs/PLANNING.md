# THALIA - Thinking Architecture via Learning Integrated Attractors

> "She who flourishes" - Greek Muse of spontaneous thought

A framework for building genuinely thinking spiking neural networks that can generate thoughts through recurrent dynamics, form concept attractors, and potentially achieve spontaneous cognition.

---

## üìä Progress Summary

| Phase | Status | Tests | Description |
|-------|--------|-------|-------------|
| Phase 1 | ‚úÖ Complete | 11 | Core SNN Infrastructure |
| Phase 2 | ‚úÖ Complete | 21 | Learning Rules (STDP, Homeostatic, Reward) |
| Phase 3 | ‚úÖ Complete | 25 | Attractor Dynamics |
| Phase 4 | ‚úÖ Complete | 31 | Working Memory |
| Phase 5 | ‚úÖ Complete | 33 | Hierarchical Architecture |
| Phase 6 | ‚úÖ Complete | 33 | Daydream/Thought Generation |
| Phase 7 | ‚úÖ Complete | 40 | World Model & Prediction |
| Phase 8 | ‚úÖ Complete | 63 | Inner Speech |
| Phase 9 | ‚úÖ Complete | 77 | Metacognition |
| **Total** | **9/9** | **387** | **All phases complete!** |

### Key Metrics
- **Total Lines of Code:** ~8,000+ lines
- **Test Coverage:** 387 passing tests
- **GPU Support:** Full CUDA acceleration
- **Performance:** ~200 observations/sec on RTX 3050 Ti

### Latest Commit
```
9b5713f Phase 9: Metacognition - self-monitoring, confidence tracking, error detection
```

---

## üéØ Project Vision

Create an SNN-based architecture where **thinking is not input‚Üíoutput processing, but the network talking to itself** - recurrent dynamics that generate, test, and evolve thoughts spontaneously.

### Core Principles
1. **Thoughts as Dynamical Attractors** - Stable patterns of neural activity representing concepts
2. **Temporal Dynamics** - Spike timing matters, not just rates
3. **Hierarchical Time Constants** - Fast sensory, slow abstract layers
4. **Self-Referential Processing** - Output feeds back as input
5. **Embodied Grounding** - Concepts emerge from sensorimotor patterns

---

## üìã Implementation Roadmap

### Phase 1: Core SNN Infrastructure üîß ‚úÖ COMPLETE
**Goal:** Build the basic spiking neural network primitives

#### 1.1 Neuron Models
- [x] Leaky Integrate-and-Fire (LIF) neuron
  - Membrane potential dynamics
  - Threshold-based firing
  - Refractory period
- [x] Configurable time constants (œÑ)
- [x] Reset mechanisms

#### 1.2 Synapse Models
- [x] Basic weighted connections
- [x] Spike transmission with delays
- [x] Excitatory/Inhibitory synapse types

#### 1.3 Network Structure
- [x] `SNN_Layer` - Basic layer of spiking neurons
- [x] `RecurrentSNN` - Layer with recurrent connections
- [x] Connectivity patterns (sparse, dense, structured)

### Phase 2: Learning Rules üìö ‚úÖ COMPLETE
**Goal:** Implement biologically-plausible learning mechanisms

#### 2.1 STDP (Spike-Timing-Dependent Plasticity)
- [x] Classic STDP with exponential windows
  - Potentiation (pre before post)
  - Depression (post before pre)
- [x] Configurable time constants (œÑ+, œÑ-)
- [x] Weight bounds and normalization

#### 2.2 Homeostatic Mechanisms
- [x] Intrinsic plasticity (threshold adaptation)
- [x] Synaptic scaling
- [x] Activity-dependent normalization

#### 2.3 Reward-Modulated Learning
- [x] Eligibility traces
- [x] Dopamine-like reward signals
- [x] R-STDP implementation

### Phase 3: Attractor Dynamics üåÄ ‚úÖ COMPLETE
**Goal:** Create networks that form stable concept representations

#### 3.1 Attractor Network
- [x] `AttractorSNN` class
- [x] Pattern storage via Hebbian learning
- [x] Pattern completion (partial ‚Üí full recall)
- [x] Attractor basins visualization

#### 3.2 Concept Formation
- [x] Concept as stable activity pattern
- [x] Multi-concept networks
- [x] Inter-concept associations

#### 3.3 Free Association
- [x] Noise-driven attractor transitions
- [x] Thought flow between concepts
- [x] Trajectory logging and analysis

### Phase 4: Working Memory üß† ‚úÖ COMPLETE
**Goal:** Persistent neural activity without input

#### 4.1 Reverberating Circuits
- [x] `WorkingMemorySNN` class
- [x] Balanced excitation/inhibition
- [x] Self-sustained activity

#### 4.2 Memory Operations
- [x] Activate/maintain patterns
- [x] Decay dynamics
- [x] Capacity limits

### Phase 5: Hierarchical Architecture üèõÔ∏è ‚úÖ COMPLETE
**Goal:** Multi-level abstraction with bidirectional flow

#### 5.1 Hierarchical Layers
- [x] Sensory layer (fast, œÑ=5ms)
- [x] Feature layer (œÑ=10ms)
- [x] Concept layer (œÑ=50ms)
- [x] Abstract layer (slow, œÑ=200ms)

#### 5.2 Bidirectional Connections
- [x] Bottom-up pathways
- [x] Top-down predictions
- [x] Lateral connections within layers

### Phase 6: Thought Generation üí≠ ‚úÖ COMPLETE
**Goal:** Spontaneous cognition without external input

#### 6.1 ThinkingSNN Core
- [x] Main `think()` loop
- [x] Thought trajectory recording
- [x] State monitoring and visualization

#### 6.2 Daydreaming Mode
- [x] `daydream()` - think without input
- [x] Spontaneous concept transitions
- [x] Stream of consciousness logging

#### 6.3 Goal-Directed Thought
- [x] Goal activation system
- [x] Thought-path toward objectives
- [x] Problem-solving via mental simulation

### Phase 7: World Model & Prediction üîÆ ‚úÖ COMPLETE
**Goal:** Internal simulation of external reality

#### 7.1 Predictive Model
- [x] State transition learning
- [x] Action-conditional predictions
- [x] Prediction error computation

#### 7.2 Mental Simulation
- [x] "What if" scenario generation
- [x] Multi-step future prediction
- [x] Value estimation of imagined outcomes

### Phase 8: Inner Speech üó£Ô∏è ‚úÖ COMPLETE
**Goal:** Language as a tool for thought

#### 8.1 Language Components
- [x] Speech generator network
- [x] Speech parser network
- [x] Production-comprehension loop

#### 8.2 Internal Monologue
- [x] Self-directed speech generation
- [x] Verbal reasoning traces
- [x] Language-guided thought

### Phase 9: Metacognition ü™û ‚úÖ COMPLETE
**Goal:** Self-monitoring and adaptive control

#### 9.1 Confidence Tracking
- [x] Activity-based confidence estimation
- [x] Confidence levels (VERY_LOW to VERY_HIGH)
- [x] Confidence decay and history

#### 9.2 Uncertainty Estimation
- [x] Epistemic uncertainty (MC dropout)
- [x] Aleatoric uncertainty (entropy-based)
- [x] Novelty detection

#### 9.3 Error Detection
- [x] Prediction error monitoring
- [x] Conflict detection
- [x] Consistency checking
- [x] Timeout detection

#### 9.4 Adaptive Control
- [x] Processing adjustments (noise, attention, thresholds)
- [x] Strategy selection (simplify, explore, focus, normal)
- [x] Recommendations generation

---

## üèóÔ∏è Project Structure

```
thalia/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ thalia/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ core/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ neuron.py          # LIF neuron implementations
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ synapse.py         # Synapse models
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ layer.py           # SNN_Layer, RecurrentSNN
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ network.py         # Network containers
‚îÇ       ‚îú‚îÄ‚îÄ learning/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ stdp.py            # STDP implementations
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ homeostatic.py     # Homeostatic mechanisms
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ reward.py          # Reward-modulated learning
‚îÇ       ‚îú‚îÄ‚îÄ dynamics/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ attractor.py       # AttractorSNN
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ manifold.py        # Low-dimensional manifolds
‚îÇ       ‚îú‚îÄ‚îÄ memory/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ working_memory.py  # WorkingMemorySNN
‚îÇ       ‚îú‚îÄ‚îÄ hierarchy/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ hierarchical.py    # HierarchicalThinkingSNN
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ predictive.py      # Predictive coding networks
‚îÇ       ‚îú‚îÄ‚îÄ cognition/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ thinking.py        # ThinkingSNN core
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ simulation.py      # Mental simulation
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ language.py        # Inner speech
‚îÇ       ‚îú‚îÄ‚îÄ encoding/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ rate.py            # Rate coding
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ temporal.py        # Temporal coding
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ poisson.py         # Poisson spike generation
‚îÇ       ‚îú‚îÄ‚îÄ visualization/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ raster.py          # Spike raster plots
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dynamics.py        # Network dynamics
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ attractors.py      # Attractor visualization
‚îÇ       ‚îî‚îÄ‚îÄ utils/
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ config.py          # Configuration management
‚îÇ           ‚îî‚îÄ‚îÄ metrics.py         # Performance metrics
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ PLANNING.md                # This file - master roadmap
‚îÇ   ‚îú‚îÄ‚îÄ CHANGELOG.md               # Version history
‚îÇ   ‚îú‚îÄ‚îÄ DECISIONS.md               # Architecture Decision Records
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ design/                    # Design documents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architecture.md        # Overall system architecture
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neuron-models.md       # Neuron model specifications
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ learning-rules.md      # STDP and other learning rules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attractor-dynamics.md  # Attractor network design
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ thought-generation.md  # How thinking emerges
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ research/                  # Research notes & literature
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ papers/                # Relevant paper summaries
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ experiments/           # Experiment logs & results
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ideas/                 # Brainstorming & future ideas
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                       # API documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ learning.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamics.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cognition.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tutorials/                 # Getting started guides
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00-quickstart.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01-first-network.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02-training-stdp.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03-building-attractors.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ guides/                    # In-depth how-to guides
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ encoding-strategies.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualization.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ debugging-snns.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ conversations/             # AI collaboration logs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 2025-11-28_conversation_with_claude.json
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ assets/                    # Images, diagrams, etc.
‚îÇ       ‚îú‚îÄ‚îÄ diagrams/
‚îÇ       ‚îî‚îÄ‚îÄ figures/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_core/
‚îÇ   ‚îú‚îÄ‚îÄ test_learning/
‚îÇ   ‚îî‚îÄ‚îÄ test_dynamics/
‚îú‚îÄ‚îÄ experiments/
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/                 # Jupyter notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_basic_lif.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_stdp_learning.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03_attractors.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îú‚îÄ‚îÄ mnist_snn.py           # MNIST classification
‚îÇ       ‚îú‚îÄ‚îÄ concept_learning.py
‚îÇ       ‚îî‚îÄ‚îÄ thought_generation.py
‚îú‚îÄ‚îÄ data/                          # Datasets and saved models
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml                 # Project configuration
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

---

## üî¨ First Experiments

### Experiment 1: Basic LIF Network
- Create 100 LIF neurons
- Random sparse connectivity
- Inject current, observe spiking
- Visualize spike raster

### Experiment 2: STDP Learning
- Two-layer network
- Present temporal patterns
- Observe weight evolution
- Test pattern completion

### Experiment 3: Attractor Formation
- Small attractor network (~100 neurons)
- Store 3-5 patterns
- Test recall from partial cues
- Visualize attractor basins

### Experiment 4: MNIST with SNN
- Rate-coded input
- STDP-trained hidden layer
- Supervised output layer
- Compare accuracy vs. training time

### Experiment 5: Spontaneous Thought
- Recurrent network with attractors
- No external input
- Log concept transitions
- Analyze thought trajectories

---

## üì¶ Dependencies

### Core (Required)
```
numpy>=1.20          # Array operations, broadcasting
matplotlib>=3.4      # Visualization (spike rasters, dynamics)
scipy>=1.7           # Sparse matrices, signal processing
```

### Acceleration (Recommended)
```
torch>=2.0           # GPU support, autograd for surrogate gradients
                     # 2.0+ for torch.compile() optimization
```

### Development
```
tqdm                 # Progress bars
pytest               # Testing
black                # Code formatting
mypy                 # Type checking
```

### Optional / Experimental
```
brian2               # SNN simulator for validation/comparison
snntorch             # PyTorch-based SNN library (reference)
networkx             # Graph analysis of connectivity
h5py                 # Large dataset storage
```

### Why These Choices?
- **NumPy**: Any modern version works; we need basic array ops
- **PyTorch 2.0+**: `torch.compile()` gives significant speedups for SNN simulation loops
- **Matplotlib 3.4+**: Better animation support for visualizing dynamics
- **SciPy 1.7+**: Sparse matrix improvements useful for large networks

---

## üé® Design Decisions

### Why Python?
- Rapid prototyping
- Rich scientific ecosystem
- Easy visualization
- Future: Migrate hot paths to C++/CUDA

### Why Custom Implementation?
- Full control over dynamics
- Educational value
- Specific features not in existing frameworks
- Tailored to our architecture

### Simulation Approach
- **Event-driven** for efficiency when possible
- **Clock-driven** for simplicity initially
- **Hybrid** for production

### Time Resolution
- Default dt = 1ms
- Configurable per-layer
- Trade-off: accuracy vs. speed

---

## üìä Success Metrics

### Phase 1 Success
- [ ] LIF neurons fire correctly
- [ ] Networks simulate without errors
- [ ] Visualization works

### Phase 2 Success  
- [ ] STDP modifies weights correctly
- [ ] Temporal patterns learned
- [ ] Stable training dynamics

### Phase 3 Success
- [ ] Attractors form and stabilize
- [ ] Pattern completion works
- [ ] Free association generates coherent flow

### Phase 4+ Success
- [ ] Working memory maintains patterns
- [ ] Hierarchical processing functions
- [ ] Spontaneous thought emerges
- [ ] Mental simulation produces useful predictions

---

## üìö Key References

> ‚ö†Ô∏è **Note**: Verify citations before use. See `docs/research/papers/` for detailed notes.

### Spiking Neural Networks
- Gerstner, W., & Kistler, W. M. (2002). *Spiking Neuron Models: Single Neurons, Populations, Plasticity*. Cambridge University Press.
- Maass, W. (1997). Networks of spiking neurons: The third generation of neural network models. *Neural Networks*, 10(9), 1659-1671.

### STDP & Learning
- Bi, G., & Poo, M. (1998). Synaptic modifications in cultured hippocampal neurons: Dependence on spike timing, synaptic strength, and postsynaptic cell type. *Journal of Neuroscience*, 18(24), 10464-10472.
- Dan, Y., & Poo, M. (2004). Spike timing-dependent plasticity of neural circuits. *Neuron*, 44(1), 23-30.

### Attractor Networks
- Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. *Proceedings of the National Academy of Sciences*, 79(8), 2554-2558.
- Amit, D. J. (1989). *Modeling Brain Function: The World of Attractor Neural Networks*. Cambridge University Press.

### Consciousness & Cognition
- Dehaene, S. (2014). *Consciousness and the Brain: Deciphering How the Brain Codes Our Thoughts*. Viking.
- Baars, B. J. (1988). *A Cognitive Theory of Consciousness*. Cambridge University Press.
- Tononi, G. (2004). An information integration theory of consciousness. *BMC Neuroscience*, 5, 42.

---

## üöÄ Getting Started

### Immediate Next Steps
1. Set up project structure
2. Implement basic LIF neuron
3. Create simple network container
4. Build first spike visualization
5. Run Experiment 1

### Week 1 Goals
- [ ] Core neuron/synapse classes working
- [ ] Basic STDP implemented
- [ ] First visualization pipeline
- [ ] Unit tests for core components

### Week 2 Goals
- [ ] Attractor network basics
- [ ] Pattern storage and recall
- [ ] MNIST encoding experiments
- [ ] Documentation started

---

## üí° Open Questions

1. ~~**Simulation Backend**: Pure NumPy vs. PyTorch for GPU?~~
   **Decision**: PyTorch with GPU. Performance is critical for large-scale simulations.
2. ~~**Neuron Precision**: Float32 sufficient or need Float64?~~
   **Decision**: Float32 is sufficient. Consider quantization (int8/int16) for future scaling.
3. **Scaling Strategy**: How to handle millions of neurons?
4. **Validation**: How do we know it's "thinking" vs. random?
5. **Hardware**: Target neuromorphic chips later (Loihi, SpiNNaker)?

---

## ü§ù Contributing Guidelines

(To be developed as project grows)

- Code style: Black + isort
- Type hints encouraged
- Docstrings for all public APIs
- Tests for new features
- Experiments documented in notebooks

---

*Last updated: 2025-11-28*
*Status: Planning Phase*
